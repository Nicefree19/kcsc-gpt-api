"""
ê²½ëŸ‰í™”ëœ GPT API ì„œë²„ v2 (ë¬´ë£Œ í´ë¼ìš°ë“œìš©)
- ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™”
- íŒŒì¼ ê¸°ë°˜ ê²€ìƒ‰ (ChromaDB ì—†ì´)
- ë¹ ë¥¸ ì‹œì‘ ì‹œê°„
- ì²­í¬ ê¸°ë°˜ ì‘ë‹µ ì§€ì› (v2 features)
"""

from fastapi import FastAPI, HTTPException, Depends, Header, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.openapi.utils import get_openapi
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import json
import os
import logging
from datetime import datetime
from functools import lru_cache
import hashlib
import re
from typing import Iterator
from fastapi.responses import StreamingResponse

# í™˜ê²½ ë³€ìˆ˜
API_KEY = os.getenv("API_KEY", "your-secure-api-key-here")
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
# í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¡œ ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
SPLIT_DATA_PATH = os.getenv("SPLIT_DATA_PATH", os.path.join(SCRIPT_DIR, "standards_split"))
GPT_ACTIONS_MODE = os.getenv("GPT_ACTIONS_MODE", "false").lower() == "true"  # GPT Actions ëª¨ë“œ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=getattr(logging, LOG_LEVEL))
logger = logging.getLogger(__name__)

# FastAPI 2.0+ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œ ì‹¤í–‰
    load_data()
    logger.info(f"API server v2 started (GPT Actions Mode: {GPT_ACTIONS_MODE})")
    yield
    # ì¢…ë£Œ ì‹œ ì‹¤í–‰ (í•„ìš” ì‹œ)

# FastAPI ì•±
app = FastAPI(
    title="Korean Construction Standards API",
    description="ê²½ëŸ‰í™”ëœ í•œêµ­ ê±´ì„¤í‘œì¤€ API (v1 + v2 í†µí•©)",
    version="2.0.0",
    servers=[
        {"url": "https://kcsc-gpt-api.onrender.com", "description": "Production Server"},
        {"url": "http://localhost:8000", "description": "Local Development Server"}
    ],
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://chat.openai.com", "https://chatgpt.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# ì „ì—­ ë³€ìˆ˜
documents_cache = {}
search_index = {}
split_index = {}
section_indexes = {}  # ì„¹ì…˜ ì¸ë±ìŠ¤ ìºì‹œ
topic_cache = {}  # ì£¼ì œë³„ ìš”ì•½ ìºì‹œ

# Pydantic ëª¨ë¸
class SearchRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=500)
    search_type: str = Field("keyword", description="keyword, code, category")
    limit: int = Field(10, ge=1, le=50)

class SearchResult(BaseModel):
    code: str
    title: str
    content: str  # v1 í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
    preview: Optional[str] = None  # v2 feature
    relevance_score: float
    metadata: Optional[Dict[str, Any]] = None
    available: Optional[Dict[str, Any]] = None  # v2 feature: ì‚¬ìš© ê°€ëŠ¥í•œ ì„¹ì…˜/íŒŒíŠ¸ ì •ë³´

# ì²­í¬ ì‘ë‹µ ëª¨ë¸
class ChunkedResponse(BaseModel):
    code: str
    query: Optional[str] = None
    total_chunks: int
    current_chunk: int
    chunks: List[Dict[str, Any]]
    next_chunk: Optional[int] = None
    completed: bool

# ì„¹ì…˜ ì¸ë±ìŠ¤ ëª¨ë¸
class SectionIndex(BaseModel):
    code: str
    title: str
    total_length: int
    sections: List[Dict[str, Any]]
    formulas: List[Dict[str, Any]]
    tables: List[Dict[str, Any]]
    quick_access: Dict[str, Any]

# ì£¼ì œë³„ ìš”ì•½ ëª¨ë¸
class TopicSummary(BaseModel):
    code: str
    topic: str
    title: str
    summary: str
    key_points: List[str]
    formulas: List[str]
    tables: List[str]
    generated_at: str
    tokens: int

# v2 ì¶”ê°€ ëª¨ë¸
class StandardSummary(BaseModel):
    code: str
    title: str
    metadata: Dict[str, Any]
    preview: str
    available_sections: List[str]
    statistics: Dict[str, Any]

class StandardSection(BaseModel):
    code: str
    title: str
    section: str
    content: str

class StandardPart(BaseModel):
    code: str
    title: str
    part: int
    total_parts: int
    content: str

# ì˜ì¡´ì„±

def normalize_code(code_str):
    """ì½”ë“œë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™” - ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ ì§€ì›"""
    if not code_str:
        return code_str
    
    # 1. ëŒ€ë¬¸ìë¡œ ë³€í™˜
    normalized = code_str.upper().strip()
    
    # 2. íŠ¹ìˆ˜ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€ê²½
    normalized = normalized.replace('_', ' ').replace('-', ' ').replace('.', ' ')
    
    # 3. ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    normalized = re.sub(r'\s+', ' ', normalized).strip()
    
    # 4. í‘œì¤€ í˜•ì‹ í™•ì¸ (ì˜ˆ: KDS 14 20 01)
    match = re.match(r'(KDS|KCS|EXCS|SMCS|LHCS)\s+(\d{1,2})\s+(\d{1,2})\s+(\d{1,2})', normalized)
    if match:
        prefix = match.group(1)
        level1 = match.group(2).zfill(2)
        level2 = match.group(3).zfill(2)
        level3 = match.group(4).zfill(2)
        return f"{prefix} {level1} {level2} {level3}"
    
    # 5. ë¶™ì–´ìˆëŠ” í˜•ì‹ ì²˜ë¦¬ (ì˜ˆ: KDS142001)
    for prefix in ['KDS', 'KCS', 'EXCS', 'SMCS', 'LHCS']:
        if normalized.startswith(prefix):
            # ì ‘ë‘ì‚¬ ë’¤ì˜ ìˆ«ì ì¶”ì¶œ
            nums = normalized[len(prefix):].replace(' ', '')
            if len(nums) == 6 and nums.isdigit():
                return f"{prefix} {nums[0:2]} {nums[2:4]} {nums[4:6]}"
            elif len(nums) >= 6:
                # ìˆ«ìë§Œ ì¶”ì¶œ
                digits = ''.join(c for c in nums if c.isdigit())
                if len(digits) >= 6:
                    return f"{prefix} {digits[0:2]} {digits[2:4]} {digits[4:6]}"
    
    # 6. ì›ë³¸ ë°˜í™˜ (ì •ê·œí™” ì‹¤íŒ¨ ì‹œ)
    return normalized


async def verify_api_key(x_api_key: str = Header(None)):
    """API í‚¤ ê²€ì¦ - GPT Actions ëª¨ë“œì—ì„œëŠ” ì„ íƒì """
    if GPT_ACTIONS_MODE:
        # GPT Actions ëª¨ë“œì—ì„œëŠ” API í‚¤ ê²€ì¦ì„ ìŠ¤í‚µí•˜ê±°ë‚˜ ì™„í™”
        logger.info(f"GPT Actions request with key: {x_api_key[:10]}..." if x_api_key else "No API key")
        return x_api_key or "gpt-actions"
    else:
        # ì¼ë°˜ ëª¨ë“œì—ì„œëŠ” ì—„ê²©í•œ ê²€ì¦
        if not x_api_key or x_api_key != API_KEY:
            raise HTTPException(status_code=401, detail="Invalid API key")
        return x_api_key

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
@lru_cache()
def load_data():
    """í–¥ìƒëœ ë°ì´í„° ë¡œë”© with ìë™ í´ë°±"""
    global documents_cache, search_index, split_index
    
    try:
        # 1. ê²€ìƒ‰ ì¸ë±ìŠ¤ ë¡œë“œ (í•„ìˆ˜)
        # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
        current_dir = os.path.dirname(os.path.abspath(__file__))
        index_path = os.getenv("INDEX_PATH", os.path.join(current_dir, "search_index.json"))
        if os.path.exists(index_path):
            with open(index_path, 'r', encoding='utf-8') as f:
                search_index = json.load(f)
            logger.info(f"âœ… Search index loaded: {len(search_index.get('codes', []))} codes")
        else:
            logger.warning(f"âš ï¸ Search index not found at {index_path}")
        
        # 2. ë¶„í•  ì¸ë±ìŠ¤ ë¡œë“œ (v2)
        # ì´ë¯¸ ì ˆëŒ€ ê²½ë¡œë¡œ ì„¤ì •ë˜ì–´ ìˆìŒ
        split_index_path = os.path.join(SPLIT_DATA_PATH, "split_index.json")
        if os.path.exists(split_index_path):
            with open(split_index_path, 'r', encoding='utf-8') as f:
                split_index = json.load(f)
            logger.info(f"âœ… Split index loaded: {len(split_index.get('standards', {}))} standards")
            
            # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: split_index ê¸°ë°˜ìœ¼ë¡œ documents_cache ìƒì„±
            for code, info in split_index.get('standards', {}).items():
                normalized_code = normalize_code(code)
                documents_cache[normalized_code] = {
                    'id': normalized_code,
                    'title': info.get('title', ''),
                    'category': code.split()[0] if ' ' in code else code[:3],
                    'content': {'full': f"[Split data] {info.get('title', '')}"},
                    'metadata': {
                        'has_parts': info.get('has_parts', False),
                        'has_full': info.get('has_full', False),
                        'sections': info.get('sections', []),
                        'source': 'split_index'
                    }
                }
        else:
            logger.warning(f"âš ï¸ Split index not found at {split_index_path}")
        
        # 3. ë¬¸ì„œ ë°ì´í„° ë¡œë“œ ì‹œë„ (v1 í˜¸í™˜ì„± - ìˆìœ¼ë©´ ì¢‹ê³  ì—†ì–´ë„ ë¨)
        data_files = [
            "kcsc_structure.json",
            "kcsc_civil.json", 
            "kcsc_building.json",
            "kcsc_facility.json",
            "kcsc_excs.json"
        ]
        
        v1_loaded = 0
        for filename in data_files:
            filepath = os.path.join(current_dir, filename)
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        for doc in data.get('documents', []):
                            normalized_id = normalize_code(doc.get('id', ''))
                            documents_cache[normalized_id] = doc
                            v1_loaded += 1
                except Exception as e:
                    logger.error(f"Failed to load {filename}: {e}")
        
        # 4. ğŸ†˜ ê¸´ê¸‰ í´ë°±: ë°ì´í„°ê°€ ì—†ìœ¼ë©´ search_indexì—ì„œ ìƒì„±
        if len(documents_cache) == 0 and search_index and 'codes' in search_index:
            logger.warning("ğŸ†˜ No documents loaded, creating from search_index...")
            for code_info in search_index['codes']:
                code = normalize_code(code_info.get('code', ''))
                documents_cache[code] = {
                    'id': code,
                    'title': code_info.get('title', code),
                    'category': code.split()[0] if ' ' in code else code[:3],
                    'content': {'full': code_info.get('title', '')},
                    'metadata': {'source': 'search_index_fallback'}
                }
            logger.info(f"ğŸ†˜ Created {len(documents_cache)} fallback entries")
        
        # ìµœì¢… ë¡œê¹…
        logger.info(f"""
        ğŸ“Š Data Loading Summary:
        - Documents loaded: {len(documents_cache)}
        - Split standards: {len(split_index.get('standards', {}))}
        - V1 documents: {v1_loaded}
        - Status: {'âœ… OK' if len(documents_cache) > 0 else 'âŒ CRITICAL'}
        """)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ Critical error in load_data: {e}")
        import traceback
        logger.error(traceback.format_exc())

# ì—”ë“œí¬ì¸íŠ¸ ì‹œì‘

# ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    return {
        "name": "Korean Construction Standards API",
        "version": "2.0.0",
        "status": "operational",
        "features": ["v1_compatibility", "chunked_responses", "section_loading", "summary_preview"]
    }

@app.get("/health")
async def health_check():
    """í–¥ìƒëœ í—¬ìŠ¤ì²´í¬ with ìƒì„¸ ì§„ë‹¨"""
    status = "healthy"
    issues = []
    
    # ë¬¸ì„œ ë¡œë”© ìƒíƒœ ì²´í¬
    if len(documents_cache) == 0:
        status = "critical"
        issues.append("No documents loaded in cache")
    elif len(documents_cache) < 100:
        status = "degraded"
        issues.append(f"Only {len(documents_cache)} documents loaded (expected more)")
    
    # ì¸ë±ìŠ¤ ìƒíƒœ ì²´í¬
    if len(split_index.get('standards', {})) == 0:
        issues.append("No split standards loaded")
    
    if len(search_index.get('codes', [])) == 0:
        issues.append("No search index codes")
    
    return {
        "status": status,
        "timestamp": datetime.now().isoformat(),
        "documents_loaded": len(documents_cache),
        "standards_split": len(split_index.get('standards', {})),
        "search_index_codes": len(search_index.get('codes', [])),
        "issues": issues,
        "mode": "GPT_ACTIONS" if GPT_ACTIONS_MODE else "STANDARD",
        "data_sources": {
            "v1_files": any(doc.get('metadata', {}).get('source') != 'split_index' 
                          for doc in documents_cache.values()),
            "split_index": any(doc.get('metadata', {}).get('source') == 'split_index' 
                              for doc in documents_cache.values()),
            "fallback": any(doc.get('metadata', {}).get('source') == 'search_index_fallback' 
                           for doc in documents_cache.values())
        }
    }

@app.post("/api/v1/search")
async def search_standards(
    request: SearchRequest,
    api_key: str = Depends(verify_api_key)
):
    """ê²€ìƒ‰ ê¸°ëŠ¥ (v1 + v2 í†µí•©)"""
    results = []
    
    if request.search_type == "code":
        # ì½”ë“œ ê²€ìƒ‰ - v2 ìš°ì„ , v1 í´ë°±
        if request.query in split_index.get('standards', {}):
            # v2: ë¶„í• ëœ í‘œì¤€ì—ì„œ ê²€ìƒ‰
            std_info = split_index['standards'][request.query]
            summary = await _load_summary(request.query)
            if summary:
                results.append(SearchResult(
                    code=request.query,
                    title=std_info['title'],
                    content=summary.get('preview', '')[:500] + '...',  # v1 í˜¸í™˜ì„±
                    preview=summary.get('preview', '')[:500],  # v2 feature
                    relevance_score=1.0,
                    metadata=summary.get('metadata', {}),
                    available={
                        'has_full': std_info.get('has_full', False),
                        'has_parts': std_info.get('has_parts', False),
                        'sections': summary.get('available_sections', []),
                        'size_kb': std_info.get('size_kb', 0)
                    }
                ))
        elif request.query in documents_cache:
            # v1: ê¸°ì¡´ ìºì‹œì—ì„œ ê²€ìƒ‰
            doc = documents_cache[request.query]
            results.append(SearchResult(
                code=request.query,
                title=doc.get('title', ''),
                content=doc.get('content', {}).get('full', '')[:500] + '...',
                relevance_score=1.0,
                metadata=doc.get('metadata')
            ))
    
    elif request.search_type == "category":
        # ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
        category_docs = search_index.get('category_index', {}).get(request.query, [])
        for doc_id in category_docs[:request.limit]:
            # v2 ìš°ì„  í™•ì¸
            if doc_id in split_index.get('standards', {}):
                std_info = split_index['standards'][doc_id]
                summary = await _load_summary(doc_id)
                if summary:
                    results.append(SearchResult(
                        code=doc_id,
                        title=std_info['title'],
                        content=f"ì¹´í…Œê³ ë¦¬: {request.query}",
                        preview=summary.get('preview', '')[:200],
                        relevance_score=0.8,
                        metadata=summary.get('metadata', {}),
                        available={
                            'has_full': std_info.get('has_full', False),
                            'has_parts': std_info.get('has_parts', False),
                            'sections': summary.get('available_sections', [])
                        }
                    ))
            elif doc_id in documents_cache:
                # v1 í´ë°±
                doc = documents_cache[doc_id]
                results.append(SearchResult(
                    code=doc_id,
                    title=doc.get('title', ''),
                    content=f"ì¹´í…Œê³ ë¦¬: {request.query}",
                    relevance_score=0.8,
                    metadata=doc.get('metadata')
                ))
    
    else:  # keyword search
        query_lower = normalize_code(request.query).lower()
        
        # v2 í‘œì¤€ì—ì„œ ê²€ìƒ‰
        for code, std_info in split_index.get('standards', {}).items():
            if query_lower in std_info['title'].lower():
                summary = await _load_summary(code)
                if summary:
                    results.append((0.7, code, std_info, summary))
                    
        # v1 ë¬¸ì„œì—ì„œ ê²€ìƒ‰
        for doc_id, doc in documents_cache.items():
            if doc_id not in split_index.get('standards', {}):  # ì¤‘ë³µ ë°©ì§€
                title = doc.get('title', '').lower()
                content = doc.get('content', {}).get('full', '').lower()
                
                if query_lower in title:
                    results.append((1.0, doc_id, doc, None))
                elif query_lower in content:
                    results.append((0.5, doc_id, doc, None))
        
        # ì •ë ¬ ë° í¬ë§·íŒ…
        results.sort(key=lambda x: x[0], reverse=True)
        formatted_results = []
        
        for item in results[:request.limit]:
            if len(item) == 4:  # v2 ê²°ê³¼
                score, code, std_info, summary = item
                formatted_results.append(SearchResult(
                    code=code,
                    title=std_info['title'],
                    content=summary.get('preview', '')[:500] + '...' if summary else '',
                    preview=summary.get('preview', '')[:500] if summary else '',
                    relevance_score=score,
                    metadata=summary.get('metadata', {}) if summary else {},
                    available={
                        'has_full': std_info.get('has_full', False),
                        'has_parts': std_info.get('has_parts', False),
                        'sections': summary.get('available_sections', []) if summary else []
                    }
                ))
            else:  # v1 ê²°ê³¼
                score, doc_id, doc = item[:3]
                formatted_results.append(SearchResult(
                    code=doc_id,
                    title=doc.get('title', ''),
                    content=doc.get('content', {}).get('full', '')[:500] + '...',
                    relevance_score=score,
                    metadata=doc.get('metadata')
                ))
        
        results = formatted_results
    
    return {
        "success": True,
        "data": {"results": results, "total": len(results)},
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v1/standard/{code}")
async def get_standard_detail(
    code: str,
    api_key: str = Depends(verify_api_key)
):
    """í‘œì¤€ ìƒì„¸ ì¡°íšŒ"""
    # ì½”ë“œ ì •ê·œí™” ì ìš©
    normalized_code = normalize_code(code)
    
    if normalized_code not in documents_cache:
        raise HTTPException(status_code=404, detail=f"Standard {normalized_code} not found")
    
    doc = documents_cache[normalized_code]
    
    return {
        "success": True,
        "data": {
            "code": code,
            "title": doc.get('title', ''),
            "full_content": doc.get('content', {}).get('full', ''),
            "sections": doc.get('content', {}).get('sections', {}),
            "metadata": doc.get('metadata', {}),
            "related_standards": doc.get('metadata', {}).get('references', [])[:10]
        },
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v1/keywords")
async def get_keywords(
    prefix: Optional[str] = None,
    limit: int = 50,
    api_key: str = Depends(verify_api_key)
):
    """í‚¤ì›Œë“œ ëª©ë¡"""
    keywords = list(search_index.get('keyword_index', {}).keys())
    
    if prefix:
        keywords = [kw for kw in keywords if kw.lower().startswith(prefix.lower())]
    
    # ë¹ˆë„ìˆœ ì •ë ¬
    keywords.sort(key=lambda x: len(search_index['keyword_index'].get(x, [])), reverse=True)
    
    return {
        "success": True,
        "data": {"keywords": keywords[:limit], "total": len(keywords)},
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v1/stats")
async def get_statistics(api_key: str = Depends(verify_api_key)):
    """í†µê³„ ì •ë³´"""
    stats = split_index.get('statistics', {})
    return {
        "success": True,
        "data": {
            "total_documents": len(documents_cache),
            "total_standards_split": len(split_index.get('standards', {})),
            "total_categories": len(search_index.get('category_index', {})),
            "total_keywords": len(search_index.get('keyword_index', {})),
            "total_size_mb": stats.get('total_size_mb', 0),
            "large_documents": len(stats.get('large_documents', [])),
            "server_type": "lightweight_v2",
            "api_version": "2.0",
            "features": ["v1_compatibility", "chunked_loading", "section_access", "summary_preview"],
            "memory_usage_mb": "< 512"
        },
        "timestamp": datetime.now().isoformat()
    }

# v2 ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸: ìš”ì•½
@app.get("/api/v1/standard/{code}/summary")
async def get_standard_summary(
    code: str,
    api_key: str = Depends(verify_api_key)
):
    """í‘œì¤€ ìš”ì•½ ì •ë³´ (v2)"""
    # ì½”ë“œ ì •ê·œí™” ì ìš©
    normalized_code = normalize_code(code)
    
    summary = await _load_summary(normalized_code)
    if not summary:
        raise HTTPException(status_code=404, detail=f"Standard {normalized_code} not found")
    
    return {
        "success": True,
        "data": summary,
        "timestamp": datetime.now().isoformat()
    }

# v2 ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸: ì„¹ì…˜
@app.get("/api/v1/standard/{code}/section/{section}")
async def get_standard_section(
    code: str,
    section: str,
    api_key: str = Depends(verify_api_key)
):
    """íŠ¹ì • ì„¹ì…˜ ë‚´ìš© (v2)"""
    safe_code = code.replace(' ', '_').replace('/', '_')
    category = code.split()[0] if ' ' in code else code[:3]
    
    section_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_section_{section}.json")
    
    if not os.path.exists(section_path):
        raise HTTPException(status_code=404, detail=f"Section {section} not found")
    
    try:
        with open(section_path, 'r', encoding='utf-8') as f:
            section_data = json.load(f)
        
        return {
            "success": True,
            "data": StandardSection(**section_data),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error loading section: {e}")
        raise HTTPException(status_code=500, detail="Error loading section")

# v2 ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸: íŒŒíŠ¸
@app.get("/api/v1/standard/{code}/part/{part}")
async def get_standard_part(
    code: str,
    part: int,
    api_key: str = Depends(verify_api_key)
):
    """íŠ¹ì • íŒŒíŠ¸ ë‚´ìš© (v2)"""
    safe_code = code.replace(' ', '_').replace('/', '_')
    category = code.split()[0] if ' ' in code else code[:3]
    
    part_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_part{part}.json")
    
    if not os.path.exists(part_path):
        raise HTTPException(status_code=404, detail=f"Part {part} not found")
    
    try:
        with open(part_path, 'r', encoding='utf-8') as f:
            part_data = json.load(f)
        
        return {
            "success": True,
            "data": StandardPart(**part_data),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error loading part: {e}")
        raise HTTPException(status_code=500, detail="Error loading part")

# v2 ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸: ì •ë³´
@app.get("/api/v1/standard/{code}/info")
async def get_standard_info(
    code: str,
    api_key: str = Depends(verify_api_key)
):
    """í‘œì¤€ êµ¬ì¡° ì •ë³´ (v2)"""
    safe_code = code.replace(' ', '_').replace('/', '_')
    category = code.split()[0] if ' ' in code else code[:3]
    
    info_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_info.json")
    
    if os.path.exists(info_path):
        try:
            with open(info_path, 'r', encoding='utf-8') as f:
                info_data = json.load(f)
            
            return {
                "success": True,
                "data": info_data,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Error loading info: {e}")
    
    # info íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì •ë³´ ë°˜í™˜
    if code in split_index.get('standards', {}):
        std_info = split_index['standards'][code]
        return {
            "success": True,
            "data": {
                "code": code,
                "title": std_info['title'],
                "has_full": std_info.get('has_full', False),
                "has_parts": std_info.get('has_parts', False),
                "size_kb": std_info.get('size_kb', 0)
            },
            "timestamp": datetime.now().isoformat()
        }
    
    raise HTTPException(status_code=404, detail=f"Standard {code} not found")

# í—¬í¼ í•¨ìˆ˜
async def _load_summary(code: str) -> Optional[Dict]:
    """ìš”ì•½ íŒŒì¼ ë¡œë“œ"""
    safe_code = code.replace(' ', '_').replace('/', '_')
    category = code.split()[0] if ' ' in code else code[:3]
    
    summary_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_summary.json")
    
    if os.path.exists(summary_path):
        try:
            with open(summary_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading summary for {code}: {e}")
    
    return None

# === ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ì—”ë“œí¬ì¸íŠ¸ ===

# ì²­í¬ ì²˜ë¦¬ í´ë˜ìŠ¤
class ChunkedDocumentProcessor:
    """ëŒ€ìš©ëŸ‰ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬"""
    
    def __init__(self, chunk_size: int = 1000):
        self.chunk_size = chunk_size
        
    def estimate_tokens(self, text: str) -> int:
        """ê°„ë‹¨í•œ í† í° ì¶”ì •"""
        korean_chars = len([c for c in text if ord(c) >= 0xAC00 and ord(c) <= 0xD7A3])
        other_chars = len(text) - korean_chars
        return korean_chars * 2 + other_chars
    
    def chunk_document(self, content: str, chunk_tokens: int = 1000) -> Iterator[Dict[str, Any]]:
        """ë¬¸ì„œë¥¼ í† í° í¬ê¸°ë³„ë¡œ ì²­í¬ ë¶„í• """
        paragraphs = content.split('\n\n')
        
        current_chunk = []
        current_tokens = 0
        chunk_index = 0
        
        for para in paragraphs:
            para_tokens = self.estimate_tokens(para)
            
            if current_tokens + para_tokens <= chunk_tokens:
                current_chunk.append(para)
                current_tokens += para_tokens
            else:
                if current_chunk:
                    yield {
                        'chunk_index': chunk_index,
                        'content': '\n\n'.join(current_chunk),
                        'tokens': current_tokens,
                        'has_more': True
                    }
                    chunk_index += 1
                
                current_chunk = [para]
                current_tokens = para_tokens
        
        if current_chunk:
            yield {
                'chunk_index': chunk_index,
                'content': '\n\n'.join(current_chunk),
                'tokens': current_tokens,
                'has_more': False
            }
    
    def create_smart_chunks(self, doc: Dict[str, Any], query: str = None) -> Iterator[Dict[str, Any]]:
        """ì¿¼ë¦¬ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹"""
        content = doc.get('content', '')
        
        if query:
            relevant_sections = self._find_relevant_sections(content, query)
            for i, section in enumerate(relevant_sections):
                yield {
                    'chunk_index': i,
                    'relevance': 'high',
                    'content': section['content'],
                    'tokens': section['tokens'],
                    'section_title': section.get('title', ''),
                    'has_more': i < len(relevant_sections) - 1
                }
        else:
            yield from self.chunk_document(content)
    
    def _find_relevant_sections(self, content: str, query: str) -> list:
        """ì¿¼ë¦¬ ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°"""
        sections = []
        query_lower = query.lower()
        
        parts = content.split('\n# ')
        
        for part in parts:
            if any(keyword in part.lower() for keyword in query_lower.split()):
                sections.append({
                    'content': part[:2000],
                    'tokens': self.estimate_tokens(part[:2000]),
                    'title': part.split('\n')[0][:50]
                })
        
        return sections[:5]

# ì²­í¬ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
chunk_processor = ChunkedDocumentProcessor()

# 1. ì²­í¬ ê¸°ë°˜ ë¬¸ì„œ ë°˜í™˜
@app.get("/api/v2/standard/{code}/chunked")
async def get_chunked_document(
    code: str,
    chunk_size: int = Query(1000, description="ì²­í¬ë‹¹ ìµœëŒ€ í† í°"),
    query: Optional[str] = Query(None, description="ê²€ìƒ‰ ì¿¼ë¦¬"),
    start_chunk: int = Query(0, description="ì‹œì‘ ì²­í¬ ì¸ë±ìŠ¤"),
    api_key: str = Depends(verify_api_key)
):
    """ëŒ€ìš©ëŸ‰ ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë°˜í™˜"""
    normalized = normalize_code(code)
    
    # ì „ì²´ ë¬¸ì„œ ë¡œë“œ ì‹œë„
    safe_code = normalized.replace(' ', '_')
    category = normalized.split()[0] if ' ' in normalized else normalized[:3]
    full_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_full.json")
    
    if not os.path.exists(full_path):
        # ìš”ì•½ë³¸ìœ¼ë¡œ í´ë°±
        summary_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_summary.json")
        if os.path.exists(summary_path):
            with open(summary_path, 'r', encoding='utf-8') as f:
                doc = json.load(f)
            doc['content'] = doc.get('preview', '')
        else:
            raise HTTPException(404, f"Document {code} not found")
    else:
        with open(full_path, 'r', encoding='utf-8') as f:
            doc = json.load(f)
    
    # ì²­í¬ ìƒì„±
    chunks = list(chunk_processor.create_smart_chunks(doc, query))
    
    # í˜ì´ì§€ë„¤ì´ì…˜
    if start_chunk >= len(chunks):
        return ChunkedResponse(
            code=normalized,
            query=query,
            total_chunks=len(chunks),
            current_chunk=start_chunk,
            chunks=[],
            completed=True
        )
    
    page_chunks = chunks[start_chunk:start_chunk + 3]
    
    return ChunkedResponse(
        code=normalized,
        query=query,
        total_chunks=len(chunks),
        current_chunk=start_chunk,
        chunks=page_chunks,
        next_chunk=start_chunk + len(page_chunks) if start_chunk + len(page_chunks) < len(chunks) else None,
        completed=start_chunk + len(page_chunks) >= len(chunks)
    )

# 2. ì„¹ì…˜ ì¸ë±ìŠ¤ ë°˜í™˜
@app.get("/api/v2/standard/{code}/section-index")
async def get_section_index(
    code: str,
    api_key: str = Depends(verify_api_key)
):
    """ë¬¸ì„œì˜ ì„¹ì…˜ ì¸ë±ìŠ¤ ë°˜í™˜"""
    normalized = normalize_code(code)
    
    # ìºì‹œ í™•ì¸
    if normalized in section_indexes:
        return section_indexes[normalized]
    
    # ì„¹ì…˜ ì¸ë±ìŠ¤ ìƒì„±
    safe_code = normalized.replace(' ', '_')
    category = normalized.split()[0] if ' ' in normalized else normalized[:3]
    full_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_full.json")
    
    if not os.path.exists(full_path):
        raise HTTPException(404, f"Document {code} not found")
    
    with open(full_path, 'r', encoding='utf-8') as f:
        doc = json.load(f)
    
    content = doc.get('content', '')
    sections = _extract_sections(content)
    formulas = _extract_formulas(content)
    tables = _extract_tables(content)
    
    index = SectionIndex(
        code=normalized,
        title=doc.get('title', ''),
        total_length=len(content),
        sections=sections,
        formulas=formulas,
        tables=tables,
        quick_access={
            'key_sections': [s for s in sections if any(
                keyword in s['title'].lower() 
                for keyword in ['ì •ì°©', 'ì´ìŒ', 'ê¸¸ì´', 'ê³„ì‚°', 'ìš”êµ¬ì‚¬í•­']
            )][:5],
            'has_formulas': len(formulas) > 0,
            'formula_count': len(formulas),
            'has_tables': len(tables) > 0,
            'table_count': len(tables)
        }
    )
    
    # ìºì‹œ ì €ì¥
    section_indexes[normalized] = index
    
    return index

# 3. ì£¼ì œë³„ ìš”ì•½ ë°˜í™˜
@app.get("/api/v2/standard/{code}/topic/{topic}")
async def get_topic_summary(
    code: str,
    topic: str,
    api_key: str = Depends(verify_api_key)
):
    """ì£¼ì œë³„ ìš”ì•½ ë°˜í™˜"""
    normalized = normalize_code(code)
    cache_key = f"{normalized}:{topic}"
    
    # ìºì‹œ í™•ì¸
    if cache_key in topic_cache:
        return topic_cache[cache_key]
    
    # ì‚¬ì „ ì •ì˜ëœ ì£¼ì œ
    common_topics = {
        'ì •ì°©ê¸¸ì´': {
            'keywords': ['ì •ì°©', 'ì •ì°©ê¸¸ì´', 'ë¬»í˜ê¸¸ì´', 'anchorage'],
            'sections': ['ì •ì°© ë° ì´ìŒ', 'ì² ê·¼ì˜ ì •ì°©', 'ì¸ì¥ì² ê·¼']
        },
        'ì´ìŒê¸¸ì´': {
            'keywords': ['ì´ìŒ', 'ì´ìŒê¸¸ì´', 'ê²¹ì¹¨ì´ìŒ', 'splice'],
            'sections': ['ì´ìŒ', 'ì² ê·¼ì˜ ì´ìŒ', 'ê²¹ì¹¨ì´ìŒ']
        },
        'í”¼ë³µë‘ê»˜': {
            'keywords': ['í”¼ë³µ', 'í”¼ë³µë‘ê»˜', 'ìµœì†Œí”¼ë³µë‘ê»˜'],
            'sections': ['í”¼ë³µë‘ê»˜', 'ì½˜í¬ë¦¬íŠ¸ í”¼ë³µ']
        },
        'ì „ë‹¨': {
            'keywords': ['ì „ë‹¨', 'ì „ë‹¨ë ¥', 'ì „ë‹¨ì² ê·¼'],
            'sections': ['ì „ë‹¨ì„¤ê³„', 'ì „ë‹¨ë³´ê°•']
        },
        'ê· ì—´': {
            'keywords': ['ê· ì—´', 'ê· ì—´í­', 'ê· ì—´ì œì–´'],
            'sections': ['ê· ì—´ì œì–´', 'ì‚¬ìš©ì„±']
        }
    }
    
    if topic not in common_topics:
        return {
            "error": "Topic not found",
            "available_topics": list(common_topics.keys())
        }
    
    # ë¬¸ì„œ ë¡œë“œ
    safe_code = normalized.replace(' ', '_')
    category = normalized.split()[0] if ' ' in normalized else normalized[:3]
    full_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_full.json")
    
    if not os.path.exists(full_path):
        # ìš”ì•½ë³¸ìœ¼ë¡œ ì‹œë„
        summary_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_summary.json")
        if os.path.exists(summary_path):
            with open(summary_path, 'r', encoding='utf-8') as f:
                doc = json.load(f)
            content = doc.get('preview', '')
        else:
            raise HTTPException(404, f"Document {code} not found")
    else:
        with open(full_path, 'r', encoding='utf-8') as f:
            doc = json.load(f)
        content = doc.get('content', '')
    
    # ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ
    topic_config = common_topics[topic]
    relevant_content = _extract_relevant_content(
        content, 
        topic_config['keywords'],
        topic_config.get('sections', [])
    )
    
    # ìš”ì•½ ìƒì„±
    summary = TopicSummary(
        code=normalized,
        topic=topic,
        title=f"{doc.get('title', '')} - {topic}",
        summary=_create_topic_summary(topic, relevant_content),
        key_points=_extract_key_points(relevant_content),
        formulas=_extract_topic_formulas(relevant_content),
        tables=_extract_topic_tables(relevant_content),
        generated_at=datetime.now().isoformat(),
        tokens=len(relevant_content) // 3
    )
    
    # ìºì‹œ ì €ì¥
    topic_cache[cache_key] = summary
    
    return summary

# 4. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
@app.get("/api/v2/standard/{code}/stream")
async def stream_document(
    code: str,
    chunk_tokens: int = Query(500, description="ì²­í¬ë‹¹ í† í°"),
    api_key: str = Depends(verify_api_key)
):
    """ë¬¸ì„œë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜"""
    normalized = normalize_code(code)
    
    safe_code = normalized.replace(' ', '_')
    category = normalized.split()[0] if ' ' in normalized else normalized[:3]
    full_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_full.json")
    
    if not os.path.exists(full_path):
        raise HTTPException(404, f"Document {code} not found")
    
    with open(full_path, 'r', encoding='utf-8') as f:
        doc = json.load(f)
    
    def generate():
        """ìŠ¤íŠ¸ë¦¬ë° ìƒì„±ê¸°"""
        for chunk in chunk_processor.chunk_document(doc.get('content', ''), chunk_tokens):
            yield json.dumps(chunk, ensure_ascii=False) + "\n"
    
    return StreamingResponse(
        generate(),
        media_type="application/x-ndjson"
    )

# í—¬í¼ í•¨ìˆ˜ë“¤
def _extract_sections(content: str) -> List[Dict[str, Any]]:
    """ì„¹ì…˜ ì¶”ì¶œ"""
    sections = []
    pattern = r'^(\d+\.?\d*)\s+(.+?)$'
    
    lines = content.split('\n')
    current_section = None
    section_content = []
    
    for i, line in enumerate(lines):
        match = re.match(pattern, line, re.MULTILINE)
        if match:
            if current_section:
                sections.append({
                    'id': current_section['id'],
                    'title': current_section['title'],
                    'start_line': current_section['start'],
                    'end_line': i,
                    'content_preview': '\n'.join(section_content[:5]),
                    'tokens': len('\n'.join(section_content)) // 3
                })
            
            current_section = {
                'id': match.group(1),
                'title': match.group(2),
                'start': i
            }
            section_content = []
        else:
            section_content.append(line)
    
    if current_section:
        sections.append({
            'id': current_section['id'],
            'title': current_section['title'],
            'start_line': current_section['start'],
            'end_line': len(lines),
            'content_preview': '\n'.join(section_content[:5]),
            'tokens': len('\n'.join(section_content)) // 3
        })
    
    return sections[:20]

def _extract_formulas(content: str) -> List[Dict[str, Any]]:
    """ìˆ˜ì‹ ì¶”ì¶œ"""
    formulas = []
    
    math_patterns = [
        r'\$\$(.+?)\$\$',  # Display math
        r'\$(.+?)\$',      # Inline math
        r'\\begin\{equation\}(.+?)\\end\{equation\}',
        r'ì‹\s*\((\d+\.?\d*)\)',  # í•œê¸€ ìˆ˜ì‹ ë²ˆí˜¸
    ]
    
    for pattern in math_patterns:
        matches = re.finditer(pattern, content, re.DOTALL)
        for match in matches:
            formulas.append({
                'formula': match.group(1) if len(match.groups()) > 0 else match.group(0),
                'position': match.start(),
                'context': content[max(0, match.start()-50):match.end()+50]
            })
    
    return formulas[:20]

def _extract_tables(content: str) -> List[Dict[str, Any]]:
    """í‘œ ì¶”ì¶œ"""
    tables = []
    
    table_patterns = [
        r'í‘œ\s*(\d+\.?\d*)',
        r'Table\s*(\d+\.?\d*)',
        r'<í‘œ\s*(\d+\.?\d*)>',
    ]
    
    for pattern in table_patterns:
        matches = re.finditer(pattern, content)
        for match in matches:
            tables.append({
                'table_id': match.group(1),
                'position': match.start(),
                'context': content[max(0, match.start()-100):match.start()+200]
            })
    
    return tables[:20]

def _extract_relevant_content(content: str, keywords: list, sections: list) -> str:
    """ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ"""
    relevant_parts = []
    lines = content.split('\n')
    
    for i, line in enumerate(lines):
        line_lower = line.lower()
        if any(kw.lower() in line_lower for kw in keywords):
            start = max(0, i - 5)
            end = min(len(lines), i + 10)
            relevant_parts.append('\n'.join(lines[start:end]))
    
    return '\n\n---\n\n'.join(relevant_parts[:10])

def _create_topic_summary(topic: str, content: str) -> str:
    """ì£¼ì œë³„ ìš”ì•½ ìƒì„±"""
    templates = {
        'ì •ì°©ê¸¸ì´': """ì² ê·¼ì˜ ì •ì°©ê¸¸ì´ëŠ” ì² ê·¼ì´ ì½˜í¬ë¦¬íŠ¸ì— ì¶©ë¶„íˆ ë¬»í˜€ ìˆì–´ 
ì„¤ê³„ ì‘ë ¥ì„ ì•ˆì „í•˜ê²Œ ì „ë‹¬í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ê¸¸ì´ì…ë‹ˆë‹¤. 
ê¸°ë³¸ ì •ì°©ê¸¸ì´ëŠ” ì² ê·¼ ì§ê²½, ì½˜í¬ë¦¬íŠ¸ ê°•ë„, ì² ê·¼ ìœ„ì¹˜ ë“±ì— ë”°ë¼ ê²°ì •ë©ë‹ˆë‹¤.""",
        
        'ì´ìŒê¸¸ì´': """ì² ê·¼ì˜ ì´ìŒê¸¸ì´ëŠ” ë‘ ì² ê·¼ì´ ê²¹ì³ì ¸ ì‘ë ¥ì„ ì „ë‹¬í•˜ëŠ” ë° 
í•„ìš”í•œ ìµœì†Œ ê¸¸ì´ì…ë‹ˆë‹¤. ì´ìŒê¸¸ì´ëŠ” ì •ì°©ê¸¸ì´ë³´ë‹¤ ê¸¸ê²Œ ì„¤ê³„ë˜ë©°, 
ì² ê·¼ ê°„ê²©ê³¼ ìœ„ì¹˜ì— ë”°ë¼ ë³´ì •ê³„ìˆ˜ê°€ ì ìš©ë©ë‹ˆë‹¤.""",
        
        'í”¼ë³µë‘ê»˜': """ì½˜í¬ë¦¬íŠ¸ í”¼ë³µë‘ê»˜ëŠ” ì² ê·¼ í‘œë©´ì—ì„œ ì½˜í¬ë¦¬íŠ¸ í‘œë©´ê¹Œì§€ì˜ 
ìµœë‹¨ê±°ë¦¬ë¡œ, ì² ê·¼ì˜ ë¶€ì‹ ë°©ì§€ì™€ ë‚´í™”ì„±ëŠ¥ í™•ë³´ë¥¼ ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.""",
        
        'ì „ë‹¨': """ì „ë‹¨ì„¤ê³„ëŠ” ë³´ì™€ ìŠ¬ë˜ë¸Œì—ì„œ ì „ë‹¨ë ¥ì— ì €í•­í•˜ê¸° ìœ„í•œ 
ì² ê·¼ ë°°ì¹˜ë¥¼ ê²°ì •í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì½˜í¬ë¦¬íŠ¸ì˜ ì „ë‹¨ê°•ë„ì™€ 
ì „ë‹¨ì² ê·¼ì˜ ê¸°ì—¬ë¥¼ ê³ ë ¤í•˜ì—¬ ì„¤ê³„í•©ë‹ˆë‹¤.""",
        
        'ê· ì—´': """ê· ì—´ì œì–´ëŠ” ì½˜í¬ë¦¬íŠ¸ êµ¬ì¡°ë¬¼ì˜ ì‚¬ìš©ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•´ 
ê· ì—´í­ì„ ì œí•œí•˜ëŠ” ì„¤ê³„ ê³¼ì •ì…ë‹ˆë‹¤. ì² ê·¼ ê°„ê²©ê³¼ í”¼ë³µë‘ê»˜ê°€ 
ì£¼ìš” ì„¤ê³„ë³€ìˆ˜ì…ë‹ˆë‹¤."""
    }
    
    base_summary = templates.get(topic, "ê´€ë ¨ ë‚´ìš© ìš”ì•½")
    
    # ì‹¤ì œ ë‚´ìš©ì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
    key_sentences = []
    for line in content.split('\n'):
        if len(line) > 20 and any(kw in line for kw in ['ê¸°ì¤€', 'ì´ìƒ', 'ì´í•˜', 'mm']):
            key_sentences.append(line.strip())
    
    if key_sentences:
        base_summary += "\n\nì£¼ìš” ê·œì •:\n" + '\n'.join(key_sentences[:3])
    
    return base_summary

def _extract_key_points(content: str) -> List[str]:
    """í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ"""
    key_points = []
    
    number_pattern = r'(\d+\.?\d*)\s*(mm|MPa|m|cm)'
    
    for line in content.split('\n'):
        if re.search(number_pattern, line):
            key_points.append(line.strip())
    
    return key_points[:5]

def _extract_topic_formulas(content: str) -> List[str]:
    """ì£¼ì œ ê´€ë ¨ ìˆ˜ì‹ ì¶”ì¶œ"""
    formulas = []
    
    formula_patterns = [
        r'[lL]_?d\s*=',  # ì •ì°©ê¸¸ì´
        r'[lL]_?s\s*=',  # ì´ìŒê¸¸ì´
        r'=\s*\d+\.?\d*\s*[Ã—x]\s*d_?b',  # ì§ê²½ ë°°ìˆ˜
    ]
    
    for pattern in formula_patterns:
        matches = re.finditer(pattern, content)
        for match in matches:
            line_start = content.rfind('\n', 0, match.start()) + 1
            line_end = content.find('\n', match.end())
            if line_end == -1:
                line_end = len(content)
            
            formula_line = content[line_start:line_end].strip()
            formulas.append(formula_line)
    
    return formulas[:3]

def _extract_topic_tables(content: str) -> List[str]:
    """ì£¼ì œ ê´€ë ¨ í‘œ ì¶”ì¶œ"""
    tables = []
    
    table_patterns = ['í‘œ', 'Table']
    for pattern in table_patterns:
        if pattern in content:
            for line in content.split('\n'):
                if pattern in line and len(line) < 100:
                    tables.append(line.strip())
    
    return tables[:3]

# OpenAPI ìŠ¤í‚¤ë§ˆ ì»¤ìŠ¤í„°ë§ˆì´ì§•
def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    
    openapi_schema = get_openapi(
        title=app.title,
        version=app.version,
        description=app.description,
        routes=app.routes,
    )
    
    # ì„œë²„ ì •ë³´ ì¶”ê°€
    openapi_schema["servers"] = [
        {"url": "https://kcsc-gpt-api.onrender.com", "description": "Production Server"}
    ]
    
    # API Key ë³´ì•ˆ ìŠ¤í‚¤ë§ˆ ì¶”ê°€
    openapi_schema["components"]["securitySchemes"] = {
        "ApiKeyAuth": {
            "type": "apiKey",
            "in": "header",
            "name": "X-API-Key"
        }
    }
    
    # ì „ì—­ ë³´ì•ˆ ì„¤ì •
    openapi_schema["security"] = [{"ApiKeyAuth": []}]
    
    app.openapi_schema = openapi_schema
    return app.openapi_schema

# ===== ê²½ëŸ‰í™” ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ =====

def estimate_tokens(text: str) -> int:
    """ê°„ë‹¨í•œ í† í° ì¶”ì • (í•œê¸€ 2í† í°, ì˜ë¬¸ 1í† í° ê¸°ì¤€)"""
    if not text:
        return 0
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    other_chars = len(text) - korean_chars
    return korean_chars * 2 + other_chars

def create_micro_summary(doc: Dict) -> Dict:
    """ì´ˆê²½ëŸ‰ ìš”ì•½ ìƒì„± (50í† í° ì´í•˜)"""
    return {
        "code": doc.get('id', ''),
        "title": doc.get('title', '')[:50],
        "tokens": 50,
        "level": "micro"
    }

def create_mini_summary(doc: Dict, max_chars: int = 200) -> Dict:
    """ë¯¸ë‹ˆ ìš”ì•½ ìƒì„± (200í† í°)"""
    content = doc.get('content', {})
    if isinstance(content, dict):
        text = content.get('full', '')[:max_chars]
    else:
        text = str(content)[:max_chars]
    
    tokens = estimate_tokens(text)
    
    return {
        "code": doc.get('id', ''),
        "title": doc.get('title', ''),
        "summary": text + ("..." if len(str(content)) > max_chars else ""),
        "tokens": tokens,
        "level": "mini"
    }

def extract_key_sections(doc: Dict, keywords: List[str], max_tokens: int = 1000) -> List[Dict]:
    """í‚¤ì›Œë“œ ê´€ë ¨ í•µì‹¬ ì„¹ì…˜ ì¶”ì¶œ"""
    sections = []
    current_tokens = 0
    
    content = doc.get('content', {})
    if isinstance(content, str):
        # ê°„ë‹¨í•œ ì„¹ì…˜ ë¶„í•  (ë¬¸ë‹¨ ê¸°ì¤€)
        paragraphs = content.split('\n\n')
        for i, para in enumerate(paragraphs):
            # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
            para_lower = para.lower()
            if any(kw.lower() in para_lower for kw in keywords):
                para_tokens = estimate_tokens(para)
                if current_tokens + para_tokens <= max_tokens:
                    sections.append({
                        "section_id": f"para_{i}",
                        "content": para[:500],
                        "tokens": para_tokens
                    })
                    current_tokens += para_tokens
                else:
                    break
    
    return sections

@app.get("/api/v2/lightweight/{code}")
async def get_lightweight_response(
    code: str,
    max_tokens: int = Query(2000, description="Maximum tokens in response"),
    level: str = Query("smart", description="Response level: micro, mini, smart"),
    api_key: str = Depends(verify_api_key)
):
    """ê²½ëŸ‰í™”ëœ ì‘ë‹µ - GPT í† í° ì œí•œ ê³ ë ¤"""
    normalized_code = normalize_code(code)
    
    # ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡°
    response = {
        "success": True,
        "code": normalized_code,
        "max_tokens": max_tokens,
        "tokens_used": 0
    }
    
    # ë¬¸ì„œ ì°¾ê¸° (ìºì‹œ ìš°ì„ , split_index í´ë°±)
    doc = None
    if normalized_code in documents_cache:
        doc = documents_cache[normalized_code]
    elif normalized_code in split_index.get('standards', {}):
        # split_indexì—ì„œ ê¸°ë³¸ ì •ë³´ ìƒì„±
        std_info = split_index['standards'][normalized_code]
        doc = {
            'id': normalized_code,
            'title': std_info.get('title', ''),
            'content': {'full': f"[Split data available] {std_info.get('title', '')}"},
            'metadata': std_info
        }
    
    if not doc:
        raise HTTPException(status_code=404, detail=f"Standard {normalized_code} not found")
    
    # ë ˆë²¨ë³„ ì‘ë‹µ ìƒì„±
    if level == "micro":
        response["data"] = create_micro_summary(doc)
        response["tokens_used"] = 50
    
    elif level == "mini":
        response["data"] = create_mini_summary(doc)
        response["tokens_used"] = response["data"]["tokens"]
    
    else:  # smart (ê¸°ë³¸ê°’)
        # ë‹¨ê³„ë³„ ì»¨í…ì¸  êµ¬ì„±
        contents = []
        
        # 1. ê¸°ë³¸ ìš”ì•½ (í•„ìˆ˜)
        mini = create_mini_summary(doc, max_chars=150)
        contents.append(mini)
        response["tokens_used"] += mini["tokens"]
        
        # 2. ë©”íƒ€ë°ì´í„° ì •ë³´ (ì—¬ìœ  ìˆìœ¼ë©´)
        if response["tokens_used"] < max_tokens - 100:
            metadata = doc.get('metadata', {})
            meta_info = {
                "type": "metadata",
                "category": metadata.get('category', ''),
                "sections": metadata.get('sections', [])[:5],  # ìµœëŒ€ 5ê°œ
                "tokens": 100
            }
            contents.append(meta_info)
            response["tokens_used"] += 100
        
        # 3. split ë°ì´í„° ê°€ìš©ì„± ì •ë³´
        if normalized_code in split_index.get('standards', {}):
            std_info = split_index['standards'][normalized_code]
            availability = {
                "type": "availability",
                "has_parts": std_info.get('has_parts', False),
                "has_sections": bool(std_info.get('sections', [])),
                "total_size_kb": std_info.get('size_kb', 0),
                "tokens": 50
            }
            contents.append(availability)
            response["tokens_used"] += 50
        
        response["data"] = {
            "level": "smart",
            "contents": contents,
            "suggestions": {
                "next_actions": [],
                "available_depth": "full" if response["tokens_used"] < max_tokens * 0.5 else "limited"
            }
        }
        
        # ë‹¤ìŒ ì•¡ì…˜ ì œì•ˆ
        if response["tokens_used"] < max_tokens * 0.3:
            response["data"]["suggestions"]["next_actions"].append("ë” ìƒì„¸í•œ ì •ë³´ ìš”ì²­ ê°€ëŠ¥")
        if normalized_code in split_index.get('standards', {}):
            response["data"]["suggestions"]["next_actions"].append("ì„¹ì…˜ë³„ ìƒì„¸ ë‚´ìš© ì¡°íšŒ ê°€ëŠ¥")
    
    response["data"]["navigation"] = {
        "endpoints": {
            "summary": f"/api/v1/standard/{normalized_code}/summary",
            "sections": f"/api/v1/standard/{normalized_code}/section/{{section}}",
            "parts": f"/api/v1/standard/{normalized_code}/part/{{part}}"
        }
    }
    
    return response

@app.post("/api/v2/smart-search")
async def smart_search(
    query: str = Query(..., description="Search query"),
    intent: Optional[str] = Query(None, description="Query intent: definition, calculation, requirement"),
    max_results: int = Query(5, description="Maximum results"),
    max_tokens_per_result: int = Query(500, description="Max tokens per result"),
    api_key: str = Depends(verify_api_key)
):
    """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ - ì˜ë„ ê¸°ë°˜ ê²½ëŸ‰ ì‘ë‹µ"""
    
    # ì˜ë„ ë¶„ì„
    detected_intent = intent
    if not detected_intent:
        # ê°„ë‹¨í•œ ì˜ë„ ì¶”ì¸¡
        query_lower = query.lower()
        if any(word in query_lower for word in ['ë­ì•¼', 'ë¬´ì—‡', 'ì •ì˜', 'what']):
            detected_intent = 'definition'
        elif any(word in query_lower for word in ['ê³„ì‚°', 'ê³µì‹', 'formula']):
            detected_intent = 'calculation'
        elif any(word in query_lower for word in ['ê¸°ì¤€', 'ê·œì •', 'requirement']):
            detected_intent = 'requirement'
        else:
            detected_intent = 'general'
    
    # ì¼ë°˜ ê²€ìƒ‰ ìˆ˜í–‰
    results = []
    query_normalized = normalize_code(query)
    
    # ì½”ë“œ ì§ì ‘ ë§¤ì¹­ ì‹œë„
    if query_normalized in documents_cache:
        doc = documents_cache[query_normalized]
        result = create_mini_summary(doc, max_chars=max_tokens_per_result // 4)
        result['relevance'] = 1.0
        result['match_type'] = 'exact_code'
        results.append(result)
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰
    if len(results) < max_results:
        query_lower = query.lower()
        for code, doc in documents_cache.items():
            if len(results) >= max_results:
                break
                
            title = doc.get('title', '').lower()
            if query_lower in title or query_lower in code.lower():
                result = create_mini_summary(doc, max_chars=max_tokens_per_result // 4)
                result['relevance'] = 0.7 if query_lower in title else 0.5
                result['match_type'] = 'keyword'
                results.append(result)
    
    # ê²°ê³¼ ì •ë ¬
    results.sort(key=lambda x: x.get('relevance', 0), reverse=True)
    
    return {
        "success": True,
        "query": query,
        "intent": detected_intent,
        "results": results[:max_results],
        "total_results": len(results),
        "suggestions": {
            "refine_query": detected_intent == 'general',
            "use_code_search": len(results) == 0
        }
    }

# OpenAPI ìŠ¤í‚¤ë§ˆ ì„¤ì •
app.openapi = custom_openapi

# ë©”ì¸
if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)