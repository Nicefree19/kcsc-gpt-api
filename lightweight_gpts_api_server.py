"""
ê²½ëŸ‰í™”ëœ GPT API ì„œë²„ v2 (ë¬´ë£Œ í´ë¼ìš°ë“œìš©)
- ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™”
- íŒŒì¼ ê¸°ë°˜ ê²€ìƒ‰ (ChromaDB ì—†ì´)
- ë¹ ë¥¸ ì‹œì‘ ì‹œê°„
- ì²­í¬ ê¸°ë°˜ ì‘ë‹µ ì§€ì› (v2 features)
"""

from fastapi import FastAPI, HTTPException, Depends, Header, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.openapi.utils import get_openapi
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import json
import os
import logging
from datetime import datetime
from functools import lru_cache
import hashlib
import re

# í™˜ê²½ ë³€ìˆ˜
API_KEY = os.getenv("API_KEY", "your-secure-api-key-here")
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
# í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¡œ ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
SPLIT_DATA_PATH = os.getenv("SPLIT_DATA_PATH", os.path.join(SCRIPT_DIR, "standards_split"))
GPT_ACTIONS_MODE = os.getenv("GPT_ACTIONS_MODE", "false").lower() == "true"  # GPT Actions ëª¨ë“œ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=getattr(logging, LOG_LEVEL))
logger = logging.getLogger(__name__)

# FastAPI 2.0+ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œ ì‹¤í–‰
    load_data()
    logger.info(f"API server v2 started (GPT Actions Mode: {GPT_ACTIONS_MODE})")
    yield
    # ì¢…ë£Œ ì‹œ ì‹¤í–‰ (í•„ìš” ì‹œ)

# FastAPI ì•±
app = FastAPI(
    title="Korean Construction Standards API",
    description="ê²½ëŸ‰í™”ëœ í•œêµ­ ê±´ì„¤í‘œì¤€ API (v1 + v2 í†µí•©)",
    version="2.0.0",
    servers=[
        {"url": "https://kcsc-gpt-api.onrender.com", "description": "Production Server"},
        {"url": "http://localhost:8000", "description": "Local Development Server"}
    ],
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://chat.openai.com", "https://chatgpt.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# ì „ì—­ ë³€ìˆ˜
documents_cache = {}
search_index = {}
split_index = {}

# Pydantic ëª¨ë¸
class SearchRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=500)
    search_type: str = Field("keyword", description="keyword, code, category")
    limit: int = Field(10, ge=1, le=50)

class SearchResult(BaseModel):
    code: str
    title: str
    content: str  # v1 í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
    preview: Optional[str] = None  # v2 feature
    relevance_score: float
    metadata: Optional[Dict[str, Any]] = None
    available: Optional[Dict[str, Any]] = None  # v2 feature: ì‚¬ìš© ê°€ëŠ¥í•œ ì„¹ì…˜/íŒŒíŠ¸ ì •ë³´

# v2 ì¶”ê°€ ëª¨ë¸
class StandardSummary(BaseModel):
    code: str
    title: str
    metadata: Dict[str, Any]
    preview: str
    available_sections: List[str]
    statistics: Dict[str, Any]

class StandardSection(BaseModel):
    code: str
    title: str
    section: str
    content: str

class StandardPart(BaseModel):
    code: str
    title: str
    part: int
    total_parts: int
    content: str

# ì˜ì¡´ì„±

def normalize_code(code_str):
    """ì½”ë“œë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™” - ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ ì§€ì›"""
    if not code_str:
        return code_str
    
    # 1. ëŒ€ë¬¸ìë¡œ ë³€í™˜
    normalized = code_str.upper().strip()
    
    # 2. íŠ¹ìˆ˜ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€ê²½
    normalized = normalized.replace('_', ' ').replace('-', ' ').replace('.', ' ')
    
    # 3. ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    normalized = re.sub(r'\s+', ' ', normalized).strip()
    
    # 4. í‘œì¤€ í˜•ì‹ í™•ì¸ (ì˜ˆ: KDS 14 20 01)
    match = re.match(r'(KDS|KCS|EXCS|SMCS|LHCS)\s+(\d{1,2})\s+(\d{1,2})\s+(\d{1,2})', normalized)
    if match:
        prefix = match.group(1)
        level1 = match.group(2).zfill(2)
        level2 = match.group(3).zfill(2)
        level3 = match.group(4).zfill(2)
        return f"{prefix} {level1} {level2} {level3}"
    
    # 5. ë¶™ì–´ìˆëŠ” í˜•ì‹ ì²˜ë¦¬ (ì˜ˆ: KDS142001)
    for prefix in ['KDS', 'KCS', 'EXCS', 'SMCS', 'LHCS']:
        if normalized.startswith(prefix):
            # ì ‘ë‘ì‚¬ ë’¤ì˜ ìˆ«ì ì¶”ì¶œ
            nums = normalized[len(prefix):].replace(' ', '')
            if len(nums) == 6 and nums.isdigit():
                return f"{prefix} {nums[0:2]} {nums[2:4]} {nums[4:6]}"
            elif len(nums) >= 6:
                # ìˆ«ìë§Œ ì¶”ì¶œ
                digits = ''.join(c for c in nums if c.isdigit())
                if len(digits) >= 6:
                    return f"{prefix} {digits[0:2]} {digits[2:4]} {digits[4:6]}"
    
    # 6. ì›ë³¸ ë°˜í™˜ (ì •ê·œí™” ì‹¤íŒ¨ ì‹œ)
    return normalized


async def verify_api_key(x_api_key: str = Header(None)):
    """API í‚¤ ê²€ì¦ - GPT Actions ëª¨ë“œì—ì„œëŠ” ì„ íƒì """
    if GPT_ACTIONS_MODE:
        # GPT Actions ëª¨ë“œì—ì„œëŠ” API í‚¤ ê²€ì¦ì„ ìŠ¤í‚µí•˜ê±°ë‚˜ ì™„í™”
        logger.info(f"GPT Actions request with key: {x_api_key[:10]}..." if x_api_key else "No API key")
        return x_api_key or "gpt-actions"
    else:
        # ì¼ë°˜ ëª¨ë“œì—ì„œëŠ” ì—„ê²©í•œ ê²€ì¦
        if not x_api_key or x_api_key != API_KEY:
            raise HTTPException(status_code=401, detail="Invalid API key")
        return x_api_key

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
@lru_cache()
def load_data():
    """í–¥ìƒëœ ë°ì´í„° ë¡œë”© with ìë™ í´ë°±"""
    global documents_cache, search_index, split_index
    
    try:
        # 1. ê²€ìƒ‰ ì¸ë±ìŠ¤ ë¡œë“œ (í•„ìˆ˜)
        # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
        current_dir = os.path.dirname(os.path.abspath(__file__))
        index_path = os.getenv("INDEX_PATH", os.path.join(current_dir, "search_index.json"))
        if os.path.exists(index_path):
            with open(index_path, 'r', encoding='utf-8') as f:
                search_index = json.load(f)
            logger.info(f"âœ… Search index loaded: {len(search_index.get('codes', []))} codes")
        else:
            logger.warning(f"âš ï¸ Search index not found at {index_path}")
        
        # 2. ë¶„í•  ì¸ë±ìŠ¤ ë¡œë“œ (v2)
        # ì´ë¯¸ ì ˆëŒ€ ê²½ë¡œë¡œ ì„¤ì •ë˜ì–´ ìˆìŒ
        split_index_path = os.path.join(SPLIT_DATA_PATH, "split_index.json")
        if os.path.exists(split_index_path):
            with open(split_index_path, 'r', encoding='utf-8') as f:
                split_index = json.load(f)
            logger.info(f"âœ… Split index loaded: {len(split_index.get('standards', {}))} standards")
            
            # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: split_index ê¸°ë°˜ìœ¼ë¡œ documents_cache ìƒì„±
            for code, info in split_index.get('standards', {}).items():
                normalized_code = normalize_code(code)
                documents_cache[normalized_code] = {
                    'id': normalized_code,
                    'title': info.get('title', ''),
                    'category': code.split()[0] if ' ' in code else code[:3],
                    'content': {'full': f"[Split data] {info.get('title', '')}"},
                    'metadata': {
                        'has_parts': info.get('has_parts', False),
                        'has_full': info.get('has_full', False),
                        'sections': info.get('sections', []),
                        'source': 'split_index'
                    }
                }
        else:
            logger.warning(f"âš ï¸ Split index not found at {split_index_path}")
        
        # 3. ë¬¸ì„œ ë°ì´í„° ë¡œë“œ ì‹œë„ (v1 í˜¸í™˜ì„± - ìˆìœ¼ë©´ ì¢‹ê³  ì—†ì–´ë„ ë¨)
        data_files = [
            "kcsc_structure.json",
            "kcsc_civil.json", 
            "kcsc_building.json",
            "kcsc_facility.json",
            "kcsc_excs.json"
        ]
        
        v1_loaded = 0
        for filename in data_files:
            filepath = os.path.join(current_dir, filename)
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        for doc in data.get('documents', []):
                            normalized_id = normalize_code(doc.get('id', ''))
                            documents_cache[normalized_id] = doc
                            v1_loaded += 1
                except Exception as e:
                    logger.error(f"Failed to load {filename}: {e}")
        
        # 4. ğŸ†˜ ê¸´ê¸‰ í´ë°±: ë°ì´í„°ê°€ ì—†ìœ¼ë©´ search_indexì—ì„œ ìƒì„±
        if len(documents_cache) == 0 and search_index and 'codes' in search_index:
            logger.warning("ğŸ†˜ No documents loaded, creating from search_index...")
            for code_info in search_index['codes']:
                code = normalize_code(code_info.get('code', ''))
                documents_cache[code] = {
                    'id': code,
                    'title': code_info.get('title', code),
                    'category': code.split()[0] if ' ' in code else code[:3],
                    'content': {'full': code_info.get('title', '')},
                    'metadata': {'source': 'search_index_fallback'}
                }
            logger.info(f"ğŸ†˜ Created {len(documents_cache)} fallback entries")
        
        # ìµœì¢… ë¡œê¹…
        logger.info(f"""
        ğŸ“Š Data Loading Summary:
        - Documents loaded: {len(documents_cache)}
        - Split standards: {len(split_index.get('standards', {}))}
        - V1 documents: {v1_loaded}
        - Status: {'âœ… OK' if len(documents_cache) > 0 else 'âŒ CRITICAL'}
        """)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ Critical error in load_data: {e}")
        import traceback
        logger.error(traceback.format_exc())

# ì—”ë“œí¬ì¸íŠ¸ ì‹œì‘

# ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    return {
        "name": "Korean Construction Standards API",
        "version": "2.0.0",
        "status": "operational",
        "features": ["v1_compatibility", "chunked_responses", "section_loading", "summary_preview"]
    }

@app.get("/health")
async def health_check():
    """í–¥ìƒëœ í—¬ìŠ¤ì²´í¬ with ìƒì„¸ ì§„ë‹¨"""
    status = "healthy"
    issues = []
    
    # ë¬¸ì„œ ë¡œë”© ìƒíƒœ ì²´í¬
    if len(documents_cache) == 0:
        status = "critical"
        issues.append("No documents loaded in cache")
    elif len(documents_cache) < 100:
        status = "degraded"
        issues.append(f"Only {len(documents_cache)} documents loaded (expected more)")
    
    # ì¸ë±ìŠ¤ ìƒíƒœ ì²´í¬
    if len(split_index.get('standards', {})) == 0:
        issues.append("No split standards loaded")
    
    if len(search_index.get('codes', [])) == 0:
        issues.append("No search index codes")
    
    return {
        "status": status,
        "timestamp": datetime.now().isoformat(),
        "documents_loaded": len(documents_cache),
        "standards_split": len(split_index.get('standards', {})),
        "search_index_codes": len(search_index.get('codes', [])),
        "issues": issues,
        "mode": "GPT_ACTIONS" if GPT_ACTIONS_MODE else "STANDARD",
        "data_sources": {
            "v1_files": any(doc.get('metadata', {}).get('source') != 'split_index' 
                          for doc in documents_cache.values()),
            "split_index": any(doc.get('metadata', {}).get('source') == 'split_index' 
                              for doc in documents_cache.values()),
            "fallback": any(doc.get('metadata', {}).get('source') == 'search_index_fallback' 
                           for doc in documents_cache.values())
        }
    }

@app.post("/api/v1/search")
async def search_standards(
    request: SearchRequest,
    api_key: str = Depends(verify_api_key)
):
    """ê²€ìƒ‰ ê¸°ëŠ¥ (v1 + v2 í†µí•©)"""
    results = []
    
    if request.search_type == "code":
        # ì½”ë“œ ê²€ìƒ‰ - v2 ìš°ì„ , v1 í´ë°±
        if request.query in split_index.get('standards', {}):
            # v2: ë¶„í• ëœ í‘œì¤€ì—ì„œ ê²€ìƒ‰
            std_info = split_index['standards'][request.query]
            summary = await _load_summary(request.query)
            if summary:
                results.append(SearchResult(
                    code=request.query,
                    title=std_info['title'],
                    content=summary.get('preview', '')[:500] + '...',  # v1 í˜¸í™˜ì„±
                    preview=summary.get('preview', '')[:500],  # v2 feature
                    relevance_score=1.0,
                    metadata=summary.get('metadata', {}),
                    available={
                        'has_full': std_info.get('has_full', False),
                        'has_parts': std_info.get('has_parts', False),
                        'sections': summary.get('available_sections', []),
                        'size_kb': std_info.get('size_kb', 0)
                    }
                ))
        elif request.query in documents_cache:
            # v1: ê¸°ì¡´ ìºì‹œì—ì„œ ê²€ìƒ‰
            doc = documents_cache[request.query]
            results.append(SearchResult(
                code=request.query,
                title=doc.get('title', ''),
                content=doc.get('content', {}).get('full', '')[:500] + '...',
                relevance_score=1.0,
                metadata=doc.get('metadata')
            ))
    
    elif request.search_type == "category":
        # ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
        category_docs = search_index.get('category_index', {}).get(request.query, [])
        for doc_id in category_docs[:request.limit]:
            # v2 ìš°ì„  í™•ì¸
            if doc_id in split_index.get('standards', {}):
                std_info = split_index['standards'][doc_id]
                summary = await _load_summary(doc_id)
                if summary:
                    results.append(SearchResult(
                        code=doc_id,
                        title=std_info['title'],
                        content=f"ì¹´í…Œê³ ë¦¬: {request.query}",
                        preview=summary.get('preview', '')[:200],
                        relevance_score=0.8,
                        metadata=summary.get('metadata', {}),
                        available={
                            'has_full': std_info.get('has_full', False),
                            'has_parts': std_info.get('has_parts', False),
                            'sections': summary.get('available_sections', [])
                        }
                    ))
            elif doc_id in documents_cache:
                # v1 í´ë°±
                doc = documents_cache[doc_id]
                results.append(SearchResult(
                    code=doc_id,
                    title=doc.get('title', ''),
                    content=f"ì¹´í…Œê³ ë¦¬: {request.query}",
                    relevance_score=0.8,
                    metadata=doc.get('metadata')
                ))
    
    else:  # keyword search
        query_lower = normalize_code(request.query).lower()
        
        # v2 í‘œì¤€ì—ì„œ ê²€ìƒ‰
        for code, std_info in split_index.get('standards', {}).items():
            if query_lower in std_info['title'].lower():
                summary = await _load_summary(code)
                if summary:
                    results.append((0.7, code, std_info, summary))
                    
        # v1 ë¬¸ì„œì—ì„œ ê²€ìƒ‰
        for doc_id, doc in documents_cache.items():
            if doc_id not in split_index.get('standards', {}):  # ì¤‘ë³µ ë°©ì§€
                title = doc.get('title', '').lower()
                content = doc.get('content', {}).get('full', '').lower()
                
                if query_lower in title:
                    results.append((1.0, doc_id, doc, None))
                elif query_lower in content:
                    results.append((0.5, doc_id, doc, None))
        
        # ì •ë ¬ ë° í¬ë§·íŒ…
        results.sort(key=lambda x: x[0], reverse=True)
        formatted_results = []
        
        for item in results[:request.limit]:
            if len(item) == 4:  # v2 ê²°ê³¼
                score, code, std_info, summary = item
                formatted_results.append(SearchResult(
                    code=code,
                    title=std_info['title'],
                    content=summary.get('preview', '')[:500] + '...' if summary else '',
                    preview=summary.get('preview', '')[:500] if summary else '',
                    relevance_score=score,
                    metadata=summary.get('metadata', {}) if summary else {},
                    available={
                        'has_full': std_info.get('has_full', False),
                        'has_parts': std_info.get('has_parts', False),
                        'sections': summary.get('available_sections', []) if summary else []
                    }
                ))
            else:  # v1 ê²°ê³¼
                score, doc_id, doc = item[:3]
                formatted_results.append(SearchResult(
                    code=doc_id,
                    title=doc.get('title', ''),
                    content=doc.get('content', {}).get('full', '')[:500] + '...',
                    relevance_score=score,
                    metadata=doc.get('metadata')
                ))
        
        results = formatted_results
    
    return {
        "success": True,
        "data": {"results": results, "total": len(results)},
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v1/standard/{code}")
async def get_standard_detail(
    code: str,
    api_key: str = Depends(verify_api_key)
):
    """í‘œì¤€ ìƒì„¸ ì¡°íšŒ"""
    # ì½”ë“œ ì •ê·œí™” ì ìš©
    normalized_code = normalize_code(code)
    
    if normalized_code not in documents_cache:
        raise HTTPException(status_code=404, detail=f"Standard {normalized_code} not found")
    
    doc = documents_cache[normalized_code]
    
    return {
        "success": True,
        "data": {
            "code": code,
            "title": doc.get('title', ''),
            "full_content": doc.get('content', {}).get('full', ''),
            "sections": doc.get('content', {}).get('sections', {}),
            "metadata": doc.get('metadata', {}),
            "related_standards": doc.get('metadata', {}).get('references', [])[:10]
        },
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v1/keywords")
async def get_keywords(
    prefix: Optional[str] = None,
    limit: int = 50,
    api_key: str = Depends(verify_api_key)
):
    """í‚¤ì›Œë“œ ëª©ë¡"""
    keywords = list(search_index.get('keyword_index', {}).keys())
    
    if prefix:
        keywords = [kw for kw in keywords if kw.lower().startswith(prefix.lower())]
    
    # ë¹ˆë„ìˆœ ì •ë ¬
    keywords.sort(key=lambda x: len(search_index['keyword_index'].get(x, [])), reverse=True)
    
    return {
        "success": True,
        "data": {"keywords": keywords[:limit], "total": len(keywords)},
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v1/stats")
async def get_statistics(api_key: str = Depends(verify_api_key)):
    """í†µê³„ ì •ë³´"""
    stats = split_index.get('statistics', {})
    return {
        "success": True,
        "data": {
            "total_documents": len(documents_cache),
            "total_standards_split": len(split_index.get('standards', {})),
            "total_categories": len(search_index.get('category_index', {})),
            "total_keywords": len(search_index.get('keyword_index', {})),
            "total_size_mb": stats.get('total_size_mb', 0),
            "large_documents": len(stats.get('large_documents', [])),
            "server_type": "lightweight_v2",
            "api_version": "2.0",
            "features": ["v1_compatibility", "chunked_loading", "section_access", "summary_preview"],
            "memory_usage_mb": "< 512"
        },
        "timestamp": datetime.now().isoformat()
    }

# v2 ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸: ìš”ì•½
@app.get("/api/v1/standard/{code}/summary")
async def get_standard_summary(
    code: str,
    api_key: str = Depends(verify_api_key)
):
    """í‘œì¤€ ìš”ì•½ ì •ë³´ (v2)"""
    # ì½”ë“œ ì •ê·œí™” ì ìš©
    normalized_code = normalize_code(code)
    
    summary = await _load_summary(normalized_code)
    if not summary:
        raise HTTPException(status_code=404, detail=f"Standard {normalized_code} not found")
    
    return {
        "success": True,
        "data": summary,
        "timestamp": datetime.now().isoformat()
    }

# v2 ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸: ì„¹ì…˜
@app.get("/api/v1/standard/{code}/section/{section}")
async def get_standard_section(
    code: str,
    section: str,
    api_key: str = Depends(verify_api_key)
):
    """íŠ¹ì • ì„¹ì…˜ ë‚´ìš© (v2)"""
    safe_code = code.replace(' ', '_').replace('/', '_')
    category = code.split()[0] if ' ' in code else code[:3]
    
    section_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_section_{section}.json")
    
    if not os.path.exists(section_path):
        raise HTTPException(status_code=404, detail=f"Section {section} not found")
    
    try:
        with open(section_path, 'r', encoding='utf-8') as f:
            section_data = json.load(f)
        
        return {
            "success": True,
            "data": StandardSection(**section_data),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error loading section: {e}")
        raise HTTPException(status_code=500, detail="Error loading section")

# v2 ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸: íŒŒíŠ¸
@app.get("/api/v1/standard/{code}/part/{part}")
async def get_standard_part(
    code: str,
    part: int,
    api_key: str = Depends(verify_api_key)
):
    """íŠ¹ì • íŒŒíŠ¸ ë‚´ìš© (v2)"""
    safe_code = code.replace(' ', '_').replace('/', '_')
    category = code.split()[0] if ' ' in code else code[:3]
    
    part_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_part{part}.json")
    
    if not os.path.exists(part_path):
        raise HTTPException(status_code=404, detail=f"Part {part} not found")
    
    try:
        with open(part_path, 'r', encoding='utf-8') as f:
            part_data = json.load(f)
        
        return {
            "success": True,
            "data": StandardPart(**part_data),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error loading part: {e}")
        raise HTTPException(status_code=500, detail="Error loading part")

# v2 ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸: ì •ë³´
@app.get("/api/v1/standard/{code}/info")
async def get_standard_info(
    code: str,
    api_key: str = Depends(verify_api_key)
):
    """í‘œì¤€ êµ¬ì¡° ì •ë³´ (v2)"""
    safe_code = code.replace(' ', '_').replace('/', '_')
    category = code.split()[0] if ' ' in code else code[:3]
    
    info_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_info.json")
    
    if os.path.exists(info_path):
        try:
            with open(info_path, 'r', encoding='utf-8') as f:
                info_data = json.load(f)
            
            return {
                "success": True,
                "data": info_data,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Error loading info: {e}")
    
    # info íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì •ë³´ ë°˜í™˜
    if code in split_index.get('standards', {}):
        std_info = split_index['standards'][code]
        return {
            "success": True,
            "data": {
                "code": code,
                "title": std_info['title'],
                "has_full": std_info.get('has_full', False),
                "has_parts": std_info.get('has_parts', False),
                "size_kb": std_info.get('size_kb', 0)
            },
            "timestamp": datetime.now().isoformat()
        }
    
    raise HTTPException(status_code=404, detail=f"Standard {code} not found")

# í—¬í¼ í•¨ìˆ˜
async def _load_summary(code: str) -> Optional[Dict]:
    """ìš”ì•½ íŒŒì¼ ë¡œë“œ"""
    safe_code = code.replace(' ', '_').replace('/', '_')
    category = code.split()[0] if ' ' in code else code[:3]
    
    summary_path = os.path.join(SPLIT_DATA_PATH, category, f"{safe_code}_summary.json")
    
    if os.path.exists(summary_path):
        try:
            with open(summary_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading summary for {code}: {e}")
    
    return None

# OpenAPI ìŠ¤í‚¤ë§ˆ ì»¤ìŠ¤í„°ë§ˆì´ì§•
def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    
    openapi_schema = get_openapi(
        title=app.title,
        version=app.version,
        description=app.description,
        routes=app.routes,
    )
    
    # ì„œë²„ ì •ë³´ ì¶”ê°€
    openapi_schema["servers"] = [
        {"url": "https://kcsc-gpt-api.onrender.com", "description": "Production Server"}
    ]
    
    # API Key ë³´ì•ˆ ìŠ¤í‚¤ë§ˆ ì¶”ê°€
    openapi_schema["components"]["securitySchemes"] = {
        "ApiKeyAuth": {
            "type": "apiKey",
            "in": "header",
            "name": "X-API-Key"
        }
    }
    
    # ì „ì—­ ë³´ì•ˆ ì„¤ì •
    openapi_schema["security"] = [{"ApiKeyAuth": []}]
    
    app.openapi_schema = openapi_schema
    return app.openapi_schema

# ===== ê²½ëŸ‰í™” ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ =====

def estimate_tokens(text: str) -> int:
    """ê°„ë‹¨í•œ í† í° ì¶”ì • (í•œê¸€ 2í† í°, ì˜ë¬¸ 1í† í° ê¸°ì¤€)"""
    if not text:
        return 0
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    other_chars = len(text) - korean_chars
    return korean_chars * 2 + other_chars

def create_micro_summary(doc: Dict) -> Dict:
    """ì´ˆê²½ëŸ‰ ìš”ì•½ ìƒì„± (50í† í° ì´í•˜)"""
    return {
        "code": doc.get('id', ''),
        "title": doc.get('title', '')[:50],
        "tokens": 50,
        "level": "micro"
    }

def create_mini_summary(doc: Dict, max_chars: int = 200) -> Dict:
    """ë¯¸ë‹ˆ ìš”ì•½ ìƒì„± (200í† í°)"""
    content = doc.get('content', {})
    if isinstance(content, dict):
        text = content.get('full', '')[:max_chars]
    else:
        text = str(content)[:max_chars]
    
    tokens = estimate_tokens(text)
    
    return {
        "code": doc.get('id', ''),
        "title": doc.get('title', ''),
        "summary": text + ("..." if len(str(content)) > max_chars else ""),
        "tokens": tokens,
        "level": "mini"
    }

def extract_key_sections(doc: Dict, keywords: List[str], max_tokens: int = 1000) -> List[Dict]:
    """í‚¤ì›Œë“œ ê´€ë ¨ í•µì‹¬ ì„¹ì…˜ ì¶”ì¶œ"""
    sections = []
    current_tokens = 0
    
    content = doc.get('content', {})
    if isinstance(content, str):
        # ê°„ë‹¨í•œ ì„¹ì…˜ ë¶„í•  (ë¬¸ë‹¨ ê¸°ì¤€)
        paragraphs = content.split('\n\n')
        for i, para in enumerate(paragraphs):
            # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
            para_lower = para.lower()
            if any(kw.lower() in para_lower for kw in keywords):
                para_tokens = estimate_tokens(para)
                if current_tokens + para_tokens <= max_tokens:
                    sections.append({
                        "section_id": f"para_{i}",
                        "content": para[:500],
                        "tokens": para_tokens
                    })
                    current_tokens += para_tokens
                else:
                    break
    
    return sections

@app.get("/api/v2/lightweight/{code}")
async def get_lightweight_response(
    code: str,
    max_tokens: int = Query(2000, description="Maximum tokens in response"),
    level: str = Query("smart", description="Response level: micro, mini, smart"),
    api_key: str = Depends(verify_api_key)
):
    """ê²½ëŸ‰í™”ëœ ì‘ë‹µ - GPT í† í° ì œí•œ ê³ ë ¤"""
    normalized_code = normalize_code(code)
    
    # ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡°
    response = {
        "success": True,
        "code": normalized_code,
        "max_tokens": max_tokens,
        "tokens_used": 0
    }
    
    # ë¬¸ì„œ ì°¾ê¸° (ìºì‹œ ìš°ì„ , split_index í´ë°±)
    doc = None
    if normalized_code in documents_cache:
        doc = documents_cache[normalized_code]
    elif normalized_code in split_index.get('standards', {}):
        # split_indexì—ì„œ ê¸°ë³¸ ì •ë³´ ìƒì„±
        std_info = split_index['standards'][normalized_code]
        doc = {
            'id': normalized_code,
            'title': std_info.get('title', ''),
            'content': {'full': f"[Split data available] {std_info.get('title', '')}"},
            'metadata': std_info
        }
    
    if not doc:
        raise HTTPException(status_code=404, detail=f"Standard {normalized_code} not found")
    
    # ë ˆë²¨ë³„ ì‘ë‹µ ìƒì„±
    if level == "micro":
        response["data"] = create_micro_summary(doc)
        response["tokens_used"] = 50
    
    elif level == "mini":
        response["data"] = create_mini_summary(doc)
        response["tokens_used"] = response["data"]["tokens"]
    
    else:  # smart (ê¸°ë³¸ê°’)
        # ë‹¨ê³„ë³„ ì»¨í…ì¸  êµ¬ì„±
        contents = []
        
        # 1. ê¸°ë³¸ ìš”ì•½ (í•„ìˆ˜)
        mini = create_mini_summary(doc, max_chars=150)
        contents.append(mini)
        response["tokens_used"] += mini["tokens"]
        
        # 2. ë©”íƒ€ë°ì´í„° ì •ë³´ (ì—¬ìœ  ìˆìœ¼ë©´)
        if response["tokens_used"] < max_tokens - 100:
            metadata = doc.get('metadata', {})
            meta_info = {
                "type": "metadata",
                "category": metadata.get('category', ''),
                "sections": metadata.get('sections', [])[:5],  # ìµœëŒ€ 5ê°œ
                "tokens": 100
            }
            contents.append(meta_info)
            response["tokens_used"] += 100
        
        # 3. split ë°ì´í„° ê°€ìš©ì„± ì •ë³´
        if normalized_code in split_index.get('standards', {}):
            std_info = split_index['standards'][normalized_code]
            availability = {
                "type": "availability",
                "has_parts": std_info.get('has_parts', False),
                "has_sections": bool(std_info.get('sections', [])),
                "total_size_kb": std_info.get('size_kb', 0),
                "tokens": 50
            }
            contents.append(availability)
            response["tokens_used"] += 50
        
        response["data"] = {
            "level": "smart",
            "contents": contents,
            "suggestions": {
                "next_actions": [],
                "available_depth": "full" if response["tokens_used"] < max_tokens * 0.5 else "limited"
            }
        }
        
        # ë‹¤ìŒ ì•¡ì…˜ ì œì•ˆ
        if response["tokens_used"] < max_tokens * 0.3:
            response["data"]["suggestions"]["next_actions"].append("ë” ìƒì„¸í•œ ì •ë³´ ìš”ì²­ ê°€ëŠ¥")
        if normalized_code in split_index.get('standards', {}):
            response["data"]["suggestions"]["next_actions"].append("ì„¹ì…˜ë³„ ìƒì„¸ ë‚´ìš© ì¡°íšŒ ê°€ëŠ¥")
    
    response["data"]["navigation"] = {
        "endpoints": {
            "summary": f"/api/v1/standard/{normalized_code}/summary",
            "sections": f"/api/v1/standard/{normalized_code}/section/{{section}}",
            "parts": f"/api/v1/standard/{normalized_code}/part/{{part}}"
        }
    }
    
    return response

@app.post("/api/v2/smart-search")
async def smart_search(
    query: str = Query(..., description="Search query"),
    intent: Optional[str] = Query(None, description="Query intent: definition, calculation, requirement"),
    max_results: int = Query(5, description="Maximum results"),
    max_tokens_per_result: int = Query(500, description="Max tokens per result"),
    api_key: str = Depends(verify_api_key)
):
    """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ - ì˜ë„ ê¸°ë°˜ ê²½ëŸ‰ ì‘ë‹µ"""
    
    # ì˜ë„ ë¶„ì„
    detected_intent = intent
    if not detected_intent:
        # ê°„ë‹¨í•œ ì˜ë„ ì¶”ì¸¡
        query_lower = query.lower()
        if any(word in query_lower for word in ['ë­ì•¼', 'ë¬´ì—‡', 'ì •ì˜', 'what']):
            detected_intent = 'definition'
        elif any(word in query_lower for word in ['ê³„ì‚°', 'ê³µì‹', 'formula']):
            detected_intent = 'calculation'
        elif any(word in query_lower for word in ['ê¸°ì¤€', 'ê·œì •', 'requirement']):
            detected_intent = 'requirement'
        else:
            detected_intent = 'general'
    
    # ì¼ë°˜ ê²€ìƒ‰ ìˆ˜í–‰
    results = []
    query_normalized = normalize_code(query)
    
    # ì½”ë“œ ì§ì ‘ ë§¤ì¹­ ì‹œë„
    if query_normalized in documents_cache:
        doc = documents_cache[query_normalized]
        result = create_mini_summary(doc, max_chars=max_tokens_per_result // 4)
        result['relevance'] = 1.0
        result['match_type'] = 'exact_code'
        results.append(result)
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰
    if len(results) < max_results:
        query_lower = query.lower()
        for code, doc in documents_cache.items():
            if len(results) >= max_results:
                break
                
            title = doc.get('title', '').lower()
            if query_lower in title or query_lower in code.lower():
                result = create_mini_summary(doc, max_chars=max_tokens_per_result // 4)
                result['relevance'] = 0.7 if query_lower in title else 0.5
                result['match_type'] = 'keyword'
                results.append(result)
    
    # ê²°ê³¼ ì •ë ¬
    results.sort(key=lambda x: x.get('relevance', 0), reverse=True)
    
    return {
        "success": True,
        "query": query,
        "intent": detected_intent,
        "results": results[:max_results],
        "total_results": len(results),
        "suggestions": {
            "refine_query": detected_intent == 'general',
            "use_code_search": len(results) == 0
        }
    }

# OpenAPI ìŠ¤í‚¤ë§ˆ ì„¤ì •
app.openapi = custom_openapi

# ë©”ì¸
if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)